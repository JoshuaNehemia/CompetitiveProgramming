# TOPIC 05: Complexity Analysis
Complexity analysis helps you determine if your algorithm is fast enough to pass within the time limit **before** you even start coding. It's about counting the number of operations your code will perform relative to the input size.

## Asymptotic Notations

Asymptotic notations describe how the runtime or memory usage of your algorithm grows as the input size ($N$) gets larger.

* **Big O Notation ($O$)**: This is the most important one. It describes the **worst-case scenario** or an **upper bound** on the complexity. When we say an algorithm is $O(N^2)$, we mean its runtime will not grow faster than a quadratic function of $N$. In ICPC, you're almost always solving for the worst case.

    * *Example*: A nested loop `for i=1 to N, for j=1 to N` performs $N \times N = N^2$ operations. Its complexity is $O(N^2)$.

* **Big Omega Notation ($\Omega$)**: This describes the **best-case scenario** or a **lower bound**. It tells you the minimum number of operations the algorithm will take.

    * *Example*: Searching for an element in an array is $\Omega(1)$ because, in the best case, you find it at the very first position.

* **Big Theta Notation ($\Theta$)**: This describes a **tight bound**. It means the algorithm's growth rate is both the upper and lower bound. The best and worst cases are of the same order.

    * *Example*: A single loop from `1 to N` that does a constant amount of work inside will always perform $N$ operations. Its complexity is $\Theta(N)$ because it's never faster or slower than linear time.

> **Key Takeaway for ICPC**: Focus on **Big O**. It tells you if your solution will pass the worst-case test data.

## Time Complexity

Time complexity measures the number of elementary operations (like assignments, comparisons, arithmetic) an algorithm performs.

* **Constants and Lower-Order Terms Don't Matter**: $O(2N^2 + 5N + 7)$ simplifies to $O(N^2)$. We only care about the term that grows the fastest as $N$ becomes large.
* **Sequential Loops**: If you have two separate loops, you **add** their complexities.
    ```cpp
    // O(A)
    for (int i = 0; i < A; ++i) { ... }
    // O(B)
    for (int j = 0; j < B; ++j) { ... }
    // Total Complexity: O(A + B)
    ```
* **Nested Loops**: If you have a loop inside another, you **multiply** their complexities.
    ```cpp
    // O(A * B)
    for (int i = 0; i < A; ++i) {
      for (int j = 0; j < B; ++j) { ... }
    }
    ```
* **Logarithmic Complexity ($O(\log N)$)**: This usually happens when you repeatedly **halve the input size**. Common examples include Binary Search, operations on Segment Trees or Fenwick Trees, and some recursive algorithms.

## Space Complexity

Space complexity measures the extra memory your algorithm needs, not including the input itself.

  * **Variables (int, char, etc.)**: $O(1)$ constant space.
  * **Data Structures (arrays, vectors, maps)**: The space is proportional to the number of elements stored. An array of size $N$ is $O(N)$. A 2D array of size $N \times M$ is $O(NM)$.
  * **Recursion**: The call stack uses space. A recursive function that calls itself $N$ times (like a naive factorial) uses $O(N)$ space. A recursive function that halves the problem (like in binary search) uses $O(\log N)$ space.


### Estimating Runtime from Constraints (The ICPC Cheat Sheet )

In competitive programming, the time limit is usually **1-2 seconds**. A typical modern judge can execute about **$10^8$ operations per second**. You can use the input constraints (usually given as $N \le \dots$) to guess the required time complexity.

Hereâ€™s a general guide:

| Input Constraint ($N$) | Required Time Complexity | Common Algorithms & Data Structures |
| :--- | :--- | :--- |
| $N \le 12$ | $O(N\!)$, $O(N \cdot 2^N)$ | Brute-force with permutations, traveling salesman problem (TSP) with DP. |
| $N \le 22$ | $O(2^N)$ | Dynamic Programming with bitmask, subset-sum problems. |
| $N \le 100$ | $O(N^4)$ | Some complex DP problems with 4 states. |
| $N \le 500$ | $O(N^3)$ | Floyd-Warshall algorithm, DP with 3 states. |
| $N \le 5,000$ | $O(N^2)$ | Simple DP, bubble/insertion/selection sort, graph traversals on dense graphs. |
| $N \le 10^5$ | $O(N \log N)$ | Standard sorting (merge sort, quicksort), binary search on the answer, segment trees. |
| $N \le 10^6$ | $O(N)$ | Linear scan, prefix sums, two pointers, hashing, BFS/DFS on sparse graphs. |
| $N \ge 10^7$ | $O(\log N)$ or $O(1)$ | Binary search, math formulas, problems solvable with direct calculation. |

**Example**: If a problem states $N \le 10^5$, you immediately know an $O(N^2)$ solution (approx. $(10^5)^2 = 10^{10}$ operations) will be too slow. You must find an $O(N \log N)$ or $O(N)$ solution. This insight guides your entire problem-solving approach from the very beginning.

![Click here to get to the next topic!](../MODULE_02/06.md)